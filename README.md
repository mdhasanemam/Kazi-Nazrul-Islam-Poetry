# ğŸ“– Nazrul Subword Language Model

A subword-level LSTM language model trained on the poetry of **Kazi Nazrul Islam**, using [SentencePiece](https://github.com/google/sentencepiece) for tokenization. Designed to improve perplexity and generate Bangla text more naturally.

---

## ğŸš€ Features

- ğŸ”¤ Subword tokenization with BPE (Byte-Pair Encoding)
- ğŸ§  Two-layer LSTM model with dropout regularization
- ğŸ“‰ Tracks perplexity on training and validation sets
- âœï¸ Generates Bangla text from seed prompts
- ğŸ›  Built with TensorFlow/Keras + SentencePiece

---

## ğŸ“¦ Installation

```bash
git clone https://github.com/yourusername/nazrul-language-model.git
cd nazrul-language-model
pip install -r requirements.txt
pip install sentencepiece
````
## ğŸ“œ License

This project is licensed under the [MIT License](LICENSE).  
Feel free to use, modify, and distribute with attribution.

---

## ğŸ¤ Acknowledgements

- ğŸ“š [Kazi Nazrul Islam](https://en.wikipedia.org/wiki/Kazi_Nazrul_Islam) â€” for his literary contributions and inspiring text corpus  
- ğŸ”¤ [Google SentencePiece](https://github.com/google/sentencepiece) â€” for subword tokenization  
- ğŸ§  [TensorFlow/Keras](https://www.tensorflow.org/) â€” for model training and architecture
