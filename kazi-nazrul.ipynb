{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":11687133,"sourceType":"datasetVersion","datasetId":7335356}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-05-05T14:34:59.717335Z","iopub.execute_input":"2025-05-05T14:34:59.717717Z","iopub.status.idle":"2025-05-05T14:34:59.731536Z","shell.execute_reply.started":"2025-05-05T14:34:59.717689Z","shell.execute_reply":"2025-05-05T14:34:59.730924Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/kazi-nazrul-islam/Kazi Nazrul Islam Poems with Text and Meanings Dataset.csv\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"from tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import LSTM, Dropout, Dense\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau\nfrom tensorflow.keras.optimizers import Adam\nimport numpy as np\n\n# Model architecture: Two-layer LSTM with Dropout\nmodel = Sequential([\n    LSTM(256, return_sequences=True, input_shape=(max_len, vocab_size)),\n    Dropout(0.3),\n    LSTM(256),\n    Dropout(0.3),\n    Dense(vocab_size, activation='softmax')\n])\n\n# Compile the model\nmodel.compile(\n    loss='categorical_crossentropy',\n    optimizer=Adam(learning_rate=0.003),\n    metrics=['accuracy']\n)\n\n# Learning rate scheduler to adaptively reduce LR\nlr_scheduler = ReduceLROnPlateau(\n    monitor='val_loss',\n    factor=0.5,\n    patience=3,\n    verbose=1,\n    min_lr=1e-5\n)\n\n# Train the model\nhistory = model.fit(\n    X_train, y_train,\n    validation_data=(X_val, y_val),\n    batch_size=128,\n    epochs=50,\n    callbacks=[lr_scheduler]\n)\n\n# Compute validation perplexity\nval_loss = model.evaluate(X_val, y_val, verbose=0)[0]\nval_perplexity = np.exp(val_loss)\nprint(f\"Validation Perplexity: {val_perplexity:.2f}\")\n\n# Compute training perplexity\ntrain_loss = history.history['loss'][-1]\ntrain_perplexity = np.exp(train_loss)\nprint(f\"Training Perplexity: {train_perplexity:.2f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-05T14:37:18.831378Z","iopub.execute_input":"2025-05-05T14:37:18.832116Z","iopub.status.idle":"2025-05-05T14:37:26.942385Z","shell.execute_reply.started":"2025-05-05T14:37:18.832091Z","shell.execute_reply":"2025-05-05T14:37:26.941605Z"}},"outputs":[{"name":"stdout","text":"Epoch 1/50\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/keras/src/layers/rnn/rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(**kwargs)\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 475ms/step - accuracy: 0.0532 - loss: 4.4050 - val_accuracy: 0.1423 - val_loss: 4.1375 - learning_rate: 0.0030\nEpoch 2/50\n\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.1759 - loss: 4.0764 - val_accuracy: 0.0650 - val_loss: 4.1037 - learning_rate: 0.0030\nEpoch 3/50\n\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - accuracy: 0.0736 - loss: 3.8269 - val_accuracy: 0.0691 - val_loss: 4.0210 - learning_rate: 0.0030\nEpoch 4/50\n\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 0.0636 - loss: 3.7923 - val_accuracy: 0.1423 - val_loss: 4.0047 - learning_rate: 0.0030\nEpoch 5/50\n\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - accuracy: 0.1544 - loss: 3.6670 - val_accuracy: 0.1423 - val_loss: 4.0307 - learning_rate: 0.0030\nEpoch 6/50\n\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - accuracy: 0.1733 - loss: 3.6451 - val_accuracy: 0.1423 - val_loss: 4.0632 - learning_rate: 0.0030\nEpoch 7/50\n\u001b[1m1/2\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.1953 - loss: 3.5544\nEpoch 7: ReduceLROnPlateau reducing learning rate to 0.001500000013038516.\n\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - accuracy: 0.1837 - loss: 3.6048 - val_accuracy: 0.1423 - val_loss: 4.0717 - learning_rate: 0.0030\nEpoch 8/50\n\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - accuracy: 0.1840 - loss: 3.5582 - val_accuracy: 0.1423 - val_loss: 4.0817 - learning_rate: 0.0015\nEpoch 9/50\n\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - accuracy: 0.1937 - loss: 3.5226 - val_accuracy: 0.1423 - val_loss: 4.0940 - learning_rate: 0.0015\nEpoch 10/50\n\u001b[1m1/2\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.1484 - loss: 3.5929\nEpoch 10: ReduceLROnPlateau reducing learning rate to 0.000750000006519258.\n\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - accuracy: 0.1681 - loss: 3.5715 - val_accuracy: 0.1423 - val_loss: 4.1028 - learning_rate: 0.0015\nEpoch 11/50\n\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - accuracy: 0.1757 - loss: 3.5749 - val_accuracy: 0.1423 - val_loss: 4.1039 - learning_rate: 7.5000e-04\nEpoch 12/50\n\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - accuracy: 0.1837 - loss: 3.5408 - val_accuracy: 0.1382 - val_loss: 4.0950 - learning_rate: 7.5000e-04\nEpoch 13/50\n\u001b[1m1/2\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.1562 - loss: 3.5736\nEpoch 13: ReduceLROnPlateau reducing learning rate to 0.000375000003259629.\n\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - accuracy: 0.1679 - loss: 3.5596 - val_accuracy: 0.1423 - val_loss: 4.0949 - learning_rate: 7.5000e-04\nEpoch 14/50\n\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - accuracy: 0.1679 - loss: 3.5122 - val_accuracy: 0.1423 - val_loss: 4.0911 - learning_rate: 3.7500e-04\nEpoch 15/50\n\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - accuracy: 0.1864 - loss: 3.5241 - val_accuracy: 0.1382 - val_loss: 4.0755 - learning_rate: 3.7500e-04\nEpoch 16/50\n\u001b[1m1/2\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.1953 - loss: 3.4313\nEpoch 16: ReduceLROnPlateau reducing learning rate to 0.0001875000016298145.\n\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - accuracy: 0.1611 - loss: 3.5232 - val_accuracy: 0.1423 - val_loss: 4.0799 - learning_rate: 3.7500e-04\nEpoch 17/50\n\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - accuracy: 0.1861 - loss: 3.4808 - val_accuracy: 0.1423 - val_loss: 4.0883 - learning_rate: 1.8750e-04\nEpoch 18/50\n\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - accuracy: 0.1864 - loss: 3.5011 - val_accuracy: 0.1423 - val_loss: 4.0938 - learning_rate: 1.8750e-04\nEpoch 19/50\n\u001b[1m1/2\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.1719 - loss: 3.5607\nEpoch 19: ReduceLROnPlateau reducing learning rate to 9.375000081490725e-05.\n\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - accuracy: 0.1759 - loss: 3.5424 - val_accuracy: 0.1423 - val_loss: 4.0971 - learning_rate: 1.8750e-04\nEpoch 20/50\n\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - accuracy: 0.1916 - loss: 3.4502 - val_accuracy: 0.1423 - val_loss: 4.0978 - learning_rate: 9.3750e-05\nEpoch 21/50\n\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - accuracy: 0.1733 - loss: 3.5073 - val_accuracy: 0.1423 - val_loss: 4.0975 - learning_rate: 9.3750e-05\nEpoch 22/50\n\u001b[1m1/2\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.2031 - loss: 3.4479\nEpoch 22: ReduceLROnPlateau reducing learning rate to 4.6875000407453626e-05.\n\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - accuracy: 0.1835 - loss: 3.4824 - val_accuracy: 0.1423 - val_loss: 4.0962 - learning_rate: 9.3750e-05\nEpoch 23/50\n\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - accuracy: 0.1809 - loss: 3.4867 - val_accuracy: 0.1423 - val_loss: 4.0953 - learning_rate: 4.6875e-05\nEpoch 24/50\n\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - accuracy: 0.1866 - loss: 3.5093 - val_accuracy: 0.1423 - val_loss: 4.0940 - learning_rate: 4.6875e-05\nEpoch 25/50\n\u001b[1m1/2\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.1797 - loss: 3.5187\nEpoch 25: ReduceLROnPlateau reducing learning rate to 2.3437500203726813e-05.\n\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - accuracy: 0.1785 - loss: 3.5247 - val_accuracy: 0.1423 - val_loss: 4.0922 - learning_rate: 4.6875e-05\nEpoch 26/50\n\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - accuracy: 0.1762 - loss: 3.4969 - val_accuracy: 0.1423 - val_loss: 4.0913 - learning_rate: 2.3438e-05\nEpoch 27/50\n\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - accuracy: 0.1837 - loss: 3.4778 - val_accuracy: 0.1423 - val_loss: 4.0902 - learning_rate: 2.3438e-05\nEpoch 28/50\n\u001b[1m1/2\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.1562 - loss: 3.5584\nEpoch 28: ReduceLROnPlateau reducing learning rate to 1.1718750101863407e-05.\n\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - accuracy: 0.1707 - loss: 3.5152 - val_accuracy: 0.1423 - val_loss: 4.0891 - learning_rate: 2.3438e-05\nEpoch 29/50\n\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - accuracy: 0.1575 - loss: 3.4917 - val_accuracy: 0.1423 - val_loss: 4.0885 - learning_rate: 1.1719e-05\nEpoch 30/50\n\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - accuracy: 0.1918 - loss: 3.4561 - val_accuracy: 0.1423 - val_loss: 4.0879 - learning_rate: 1.1719e-05\nEpoch 31/50\n\u001b[1m1/2\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.1562 - loss: 3.5630\nEpoch 31: ReduceLROnPlateau reducing learning rate to 1e-05.\n\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - accuracy: 0.1707 - loss: 3.5283 - val_accuracy: 0.1423 - val_loss: 4.0873 - learning_rate: 1.1719e-05\nEpoch 32/50\n\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - accuracy: 0.1916 - loss: 3.4552 - val_accuracy: 0.1423 - val_loss: 4.0867 - learning_rate: 1.0000e-05\nEpoch 33/50\n\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - accuracy: 0.1918 - loss: 3.4953 - val_accuracy: 0.1423 - val_loss: 4.0862 - learning_rate: 1.0000e-05\nEpoch 34/50\n\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - accuracy: 0.1651 - loss: 3.5050 - val_accuracy: 0.1423 - val_loss: 4.0856 - learning_rate: 1.0000e-05\nEpoch 35/50\n\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - accuracy: 0.1649 - loss: 3.4326 - val_accuracy: 0.1423 - val_loss: 4.0850 - learning_rate: 1.0000e-05\nEpoch 36/50\n\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - accuracy: 0.1703 - loss: 3.4778 - val_accuracy: 0.1423 - val_loss: 4.0844 - learning_rate: 1.0000e-05\nEpoch 37/50\n\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - accuracy: 0.1757 - loss: 3.4657 - val_accuracy: 0.1423 - val_loss: 4.0838 - learning_rate: 1.0000e-05\nEpoch 38/50\n\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - accuracy: 0.1707 - loss: 3.4735 - val_accuracy: 0.1423 - val_loss: 4.0832 - learning_rate: 1.0000e-05\nEpoch 39/50\n\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - accuracy: 0.1679 - loss: 3.4756 - val_accuracy: 0.1423 - val_loss: 4.0826 - learning_rate: 1.0000e-05\nEpoch 40/50\n\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - accuracy: 0.1655 - loss: 3.4792 - val_accuracy: 0.1423 - val_loss: 4.0820 - learning_rate: 1.0000e-05\nEpoch 41/50\n\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - accuracy: 0.1837 - loss: 3.4417 - val_accuracy: 0.1423 - val_loss: 4.0814 - learning_rate: 1.0000e-05\nEpoch 42/50\n\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - accuracy: 0.1762 - loss: 3.4727 - val_accuracy: 0.1423 - val_loss: 4.0808 - learning_rate: 1.0000e-05\nEpoch 43/50\n\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - accuracy: 0.1811 - loss: 3.4416 - val_accuracy: 0.1423 - val_loss: 4.0802 - learning_rate: 1.0000e-05\nEpoch 44/50\n\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - accuracy: 0.1755 - loss: 3.4332 - val_accuracy: 0.1423 - val_loss: 4.0796 - learning_rate: 1.0000e-05\nEpoch 45/50\n\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - accuracy: 0.1842 - loss: 3.4781 - val_accuracy: 0.1423 - val_loss: 4.0790 - learning_rate: 1.0000e-05\nEpoch 46/50\n\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - accuracy: 0.1864 - loss: 3.4390 - val_accuracy: 0.1423 - val_loss: 4.0784 - learning_rate: 1.0000e-05\nEpoch 47/50\n\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - accuracy: 0.1864 - loss: 3.4995 - val_accuracy: 0.1423 - val_loss: 4.0778 - learning_rate: 1.0000e-05\nEpoch 48/50\n\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - accuracy: 0.1655 - loss: 3.4707 - val_accuracy: 0.1423 - val_loss: 4.0771 - learning_rate: 1.0000e-05\nEpoch 49/50\n\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - accuracy: 0.1629 - loss: 3.4885 - val_accuracy: 0.1423 - val_loss: 4.0765 - learning_rate: 1.0000e-05\nEpoch 50/50\n\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - accuracy: 0.1705 - loss: 3.4723 - val_accuracy: 0.1423 - val_loss: 4.0759 - learning_rate: 1.0000e-05\nValidation Perplexity: 58.90\nTraining Perplexity: 32.68\n","output_type":"stream"}],"execution_count":20},{"cell_type":"code","source":"import numpy as np\nperplexity = np.exp(model.evaluate(X_test, y_test, verbose=0)[0])\nprint(f\"Perplexity: {perplexity}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-05T14:37:33.888106Z","iopub.execute_input":"2025-05-05T14:37:33.888722Z","iopub.status.idle":"2025-05-05T14:37:34.001652Z","shell.execute_reply.started":"2025-05-05T14:37:33.888699Z","shell.execute_reply":"2025-05-05T14:37:34.000878Z"}},"outputs":[{"name":"stdout","text":"Perplexity: 48.31808630646097\n","output_type":"stream"}],"execution_count":21}]}